# Vertex AI job settings
project: YOUR_GCP_PROJECT_ID
region: us-central1
staging_bucket: your-staging-bucket  # bucket name only, without gs://
display_name: llama-sft-run
container_image: us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.2-3:latest
machine_type: a2-highgpu-1g
accelerator_type: NVIDIA_TESLA_A100
accelerator_count: 1

# Training data and outputs
model_id: meta-llama/Llama-3.1-8B-Instruct
train_path: training_exp/dataset/case_briefs-train-0.jsonl  # local path or gs://
# If omitted or local, launcher will place outputs under gs://<staging_bucket>/outputs/<run-id>/
# output_dir: gs://your-staging-bucket/outputs/my-run/

# Trainer hyperparameters
num_train_epochs: 1.0
learning_rate: 0.0002
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
max_seq_length: 2048
logging_steps: 10
save_steps: 200
warmup_ratio: 0.03
seed: 42
english_only: true
no_4bit: false

# LoRA config (you can also use lora_r/lora_alpha/lora_dropout top-level keys)
lora:
  r: 16
  alpha: 32
  dropout: 0.05
